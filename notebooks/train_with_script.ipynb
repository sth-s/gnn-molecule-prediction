{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abdadd5f",
   "metadata": {},
   "source": [
    "# GNN Molecular Toxicity Prediction - Training & Grid Search\n",
    "\n",
    "This notebook demonstrates training GIN models for molecular toxicity prediction using both single experiments and hyperparameter grid search.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Google Colab support**: automatic setup and dependency installation\n",
    "- **Single model training**: configurable hyperparameters and data splitting\n",
    "- **Grid search**: systematic hyperparameter optimization\n",
    "- **Comprehensive analysis**: training curves, model comparison, and results visualization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ff5b7",
   "metadata": {},
   "source": [
    "## 1. Environment Setup (Google Colab)\n",
    "\n",
    "Run the following cell to set up the environment in Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd540908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# This cell is intended to be run only in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab...\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running in local environment...\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # 1. Remove existing folder if it already exists\n",
    "    repo_name = 'gnn-molecule-prediction'\n",
    "    repo_path = os.path.join('/content', repo_name)\n",
    "\n",
    "    if os.path.exists(repo_path):\n",
    "        shutil.rmtree(repo_path)\n",
    "        print(f\"Removed existing directory: {repo_path}\")\n",
    "    else:\n",
    "        print(f\"No existing directory found.\")\n",
    "\n",
    "    # 2. Clone the GitHub repository\n",
    "    %cd /content\n",
    "    !git clone https://github.com/sth-s/gnn-molecule-prediction.git\n",
    "\n",
    "    # 3. Change working directory to the project root\n",
    "    %cd gnn-molecule-prediction\n",
    "\n",
    "    # 4. Install dependencies via pip\n",
    "    !pip install torch torch-geometric scikit-learn matplotlib seaborn deepchem\n",
    "    !pip install rdkit-pypi  # For molecular processing\n",
    "    \n",
    "    print(\"\\nDependencies installed successfully!\")\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "    \n",
    "else:\n",
    "    # For local development, just change to the parent directory\n",
    "    if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "        os.chdir('../')\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Verify the setup\n",
    "print(\"\\n=== Environment Verification ===\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Directory contents: {os.listdir('.')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167a666",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ad694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_palette('muted')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n=== Setup Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec2a67",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Basic Model Training\n",
    "\n",
    "Let's start with basic training examples using the standalone script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e9546",
   "metadata": {},
   "source": [
    "# Training GIN Model with Command-Line Script\n",
    "\n",
    "This notebook demonstrates how to use the standalone training script `scripts/train.py` for molecular toxicity prediction. The script provides a command-line interface with configurable hyperparameters and data splitting methods.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Multiple data splitting methods**: random, scaffold-based, and index-based\n",
    "- **Configurable hyperparameters**: learning rate, batch size, model architecture, etc.\n",
    "- **Automatic model saving**: saves best model and optional checkpoints\n",
    "- **Comprehensive logging**: detailed training progress and results\n",
    "- **Reproducible results**: configurable random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb4736c",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "Let's start with a basic training run using default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02268e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run basic training with default parameters\n",
    "!python ../scripts/train.py --epochs 50 --exp_name \"basic_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46243a0",
   "metadata": {},
   "source": [
    "## View Available Options\n",
    "\n",
    "Let's see all available command-line options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a21f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View help information\n",
    "!python ../scripts/train.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ee6d58",
   "metadata": {},
   "source": [
    "## Custom Hyperparameters\n",
    "\n",
    "Now let's try different hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a771ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with custom hyperparameters\n",
    "!python ../scripts/train.py \\\n",
    "    --epochs 100 \\\n",
    "    --lr 5e-4 \\\n",
    "    --hidden_channels 128 \\\n",
    "    --num_layers 4 \\\n",
    "    --dropout 0.3 \\\n",
    "    --batch_size 64 \\\n",
    "    --exp_name \"custom_hyperparams\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237df51",
   "metadata": {},
   "source": [
    "## Different Data Splitting Methods\n",
    "\n",
    "### Scaffold-based Split\n",
    "This method groups molecules by their Murcko scaffolds, ensuring better generalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1bb431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaffold-based splitting\n",
    "!python ../scripts/train.py \\\n",
    "    --epochs 75 \\\n",
    "    --split_method scaffold \\\n",
    "    --exp_name \"scaffold_split_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7786f",
   "metadata": {},
   "source": [
    "### Index-based Split\n",
    "This method splits based on molecule indices (deterministic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ad0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index-based splitting\n",
    "!python ../scripts/train.py \\\n",
    "    --epochs 75 \\\n",
    "    --split_method index \\\n",
    "    --exp_name \"index_split_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f0c069",
   "metadata": {},
   "source": [
    "## Custom Split Ratios\n",
    "\n",
    "You can also customize the train/validation/test split ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b823ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom split ratios (80/10/10)\n",
    "!python ../scripts/train.py \\\n",
    "    --epochs 50 \\\n",
    "    --train_ratio 0.8 \\\n",
    "    --val_ratio 0.1 \\\n",
    "    --test_ratio 0.1 \\\n",
    "    --exp_name \"custom_split_ratios\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88d6df",
   "metadata": {},
   "source": [
    "## Viewing Results\n",
    "\n",
    "Let's explore the saved results from our training runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88092db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# List all experiment directories\n",
    "models_dir = Path(\"../models\")\n",
    "if models_dir.exists():\n",
    "    experiments = [d.name for d in models_dir.iterdir() if d.is_dir()]\n",
    "    print(\"Available experiments:\")\n",
    "    for exp in sorted(experiments):\n",
    "        print(f\"  - {exp}\")\n",
    "else:\n",
    "    print(\"No experiments found. Run a training script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2c5d2",
   "metadata": {},
   "source": [
    "## Compare Different Experiments\n",
    "\n",
    "Let's compare the performance of different experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b27180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display results from the most recent experiment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def load_experiment_results(exp_name):\n",
    "    \"\"\"Load results from an experiment.\"\"\"\n",
    "    # results_path = models_dir / exp_name / \"results.json\"\n",
    "    results_path = Path(\"/home/sths/Documents/proj/gnn-molecule-prediction/best/results.json\")\n",
    "    if results_path.exists():\n",
    "        with open(results_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "def plot_training_curves(results):\n",
    "    \"\"\"Plot training and validation curves.\"\"\"\n",
    "    history = results['history']\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], label='Train', alpha=0.8)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], label='Validation', alpha=0.8)\n",
    "    axes[0, 0].set_title('Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ROC AUC\n",
    "    axes[0, 1].plot(epochs, history['train_roc_auc'], label='Train', alpha=0.8)\n",
    "    axes[0, 1].plot(epochs, history['val_roc_auc'], label='Validation', alpha=0.8)\n",
    "    axes[0, 1].set_title('ROC AUC')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('ROC AUC')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # PR AUC\n",
    "    axes[1, 0].plot(epochs, history['train_pr_auc'], label='Train', alpha=0.8)\n",
    "    axes[1, 0].plot(epochs, history['val_pr_auc'], label='Validation', alpha=0.8)\n",
    "    axes[1, 0].set_title('PR AUC')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('PR AUC')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning Rate\n",
    "    if history['learning_rates']:\n",
    "        axes[1, 1].plot(history['learning_rates'], alpha=0.8)\n",
    "        axes[1, 1].set_title('Learning Rate')\n",
    "        axes[1, 1].set_xlabel('Step')\n",
    "        axes[1, 1].set_ylabel('Learning Rate')\n",
    "        axes[1, 1].set_yscale('log')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No LR history', ha='center', va='center')\n",
    "        axes[1, 1].set_title('Learning Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Try to load and plot results from the most recent experiment\n",
    "# if 'experiments' in locals() and experiments:\n",
    "if True:\n",
    "    # latest_exp = sorted(experiments)[-1]\n",
    "    # print(f\"Loading results from: {latest_exp}\")\n",
    "    \n",
    "    results = load_experiment_results(\"best\")\n",
    "    if results:\n",
    "        print(f\"\\nExperiment Summary:\")\n",
    "        print(f\"  Best validation ROC AUC: {results['training_info']['best_val_roc_auc']:.4f} (epoch {results['training_info']['best_epoch']})\")\n",
    "        print(f\"  Final test ROC AUC: {results['final_results']['test']['roc_auc']:.4f}\")\n",
    "        print(f\"  Final test PR AUC: {results['final_results']['test']['pr_auc']:.4f}\")\n",
    "        print(f\"  Training time: {results['training_info']['total_time_minutes']:.1f}s\")\n",
    "        \n",
    "        # Plot training curves\n",
    "        plot_training_curves(results)\n",
    "    else:\n",
    "        print(\"Could not load results.\")\n",
    "else:\n",
    "    print(\"No experiments available to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79845550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare experiment results\n",
    "def compare_experiments():\n",
    "    \"\"\"Compare results from all experiments.\"\"\"\n",
    "    if not models_dir.exists():\n",
    "        print(\"No experiments found.\")\n",
    "        return\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for exp_dir in models_dir.iterdir():\n",
    "        if exp_dir.is_dir():\n",
    "            results = load_experiment_results(exp_dir.name)\n",
    "            if results:\n",
    "                comparison_data.append({\n",
    "                    'experiment': exp_dir.name,\n",
    "                    'split_method': results['args']['split_method'],\n",
    "                    'lr': results['args']['lr'],\n",
    "                    'hidden_channels': results['args']['hidden_channels'],\n",
    "                    'num_layers': results['args']['num_layers'],\n",
    "                    'dropout': results['args']['dropout'],\n",
    "                    'best_val_roc': results['final_results']['best_validation']['roc_auc'],\n",
    "                    'test_roc': results['test']['roc_auc'],\n",
    "                    'test_pr': results['test']['pr_auc'],\n",
    "                    'training_time': results['training_info']['total_time_minutes']\n",
    "                })\n",
    "    \n",
    "    if comparison_data:\n",
    "        import pandas as pd\n",
    "        \n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        df = df.sort_values('test_roc', ascending=False)\n",
    "        \n",
    "        print(\"Experiment Comparison (sorted by test ROC AUC):\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            print(f\"Experiment: {row['experiment']}\")\n",
    "            print(f\"  Split: {row['split_method']:8s} | LR: {row['lr']:.1e} | Hidden: {row['hidden_channels']:3d} | Layers: {row['num_layers']} | Dropout: {row['dropout']:.2f}\")\n",
    "            print(f\"  Val ROC: {row['best_val_roc']:.4f} | Test ROC: {row['test_roc']:.4f} | Test PR: {row['test_pr']:.4f} | Time: {row['training_time']:.1f}s\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No experiment results found.\")\n",
    "\n",
    "compare_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba255f",
   "metadata": {},
   "source": [
    "## Loading Saved Models\n",
    "\n",
    "You can also load the saved models for inference or further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64daf373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('../src')\n",
    "\n",
    "from model import GIN\n",
    "from data_utils import load_tox21\n",
    "\n",
    "def load_trained_model(exp_name, device='cpu'):\n",
    "    \"\"\"Load a trained model from an experiment.\"\"\"\n",
    "    exp_dir = models_dir / exp_name\n",
    "    \n",
    "    # Load configuration\n",
    "    config_path = exp_dir / \"config.json\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Load dataset to get dimensions\n",
    "    dataset = load_tox21(\n",
    "        root=config['data_root'],\n",
    "        filename=config['filename'],\n",
    "        smiles_col=\"smiles\",\n",
    "        mol_id_col=\"mol_id\",\n",
    "        cache_file=\"data.pt\",\n",
    "        recreate=False,\n",
    "        auto_download=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = GIN(\n",
    "        in_channels=dataset.num_node_features,\n",
    "        hidden_channels=config['hidden_channels'],\n",
    "        num_classes=dataset.num_classes,\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout']\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    model_path = exp_dir / \"best_model.pt\"\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    return model, dataset, config\n",
    "\n",
    "# Example: Load the best performing model\n",
    "if 'experiments' in locals() and experiments:\n",
    "    # Load the first experiment as an example\n",
    "    exp_name = experiments[0]\n",
    "    print(f\"Loading model from experiment: {exp_name}\")\n",
    "    \n",
    "    try:\n",
    "        model, dataset, config = load_trained_model(exp_name)\n",
    "        print(f\"Successfully loaded model with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "        print(f\"Model configuration: {config['hidden_channels']} hidden channels, {config['num_layers']} layers\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "else:\n",
    "    print(\"No experiments available to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded8fd1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `scripts/train.py` script provides a comprehensive command-line interface for training GIN models on molecular toxicity prediction tasks. Key features include:\n",
    "\n",
    "1. **Flexible data splitting**: Choose between random, scaffold-based, or index-based splitting\n",
    "2. **Configurable hyperparameters**: Easily adjust learning rate, model architecture, training settings\n",
    "3. **Automatic model management**: Saves best models and optional checkpoints\n",
    "4. **Comprehensive logging**: Detailed progress tracking and result saving\n",
    "5. **Reproducible experiments**: Consistent random seeding and configuration saving\n",
    "\n",
    "This approach allows for systematic hyperparameter tuning and comparison of different training configurations while maintaining compatibility with the existing notebook-based workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ccf3c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Hyperparameter Grid Search\n",
    "\n",
    "Now let's perform systematic hyperparameter optimization using our grid search script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed26fcf",
   "metadata": {},
   "source": [
    "### 4.1 Grid Search Options\n",
    "\n",
    "First, let's see the available options for grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a9aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View grid search options\n",
    "!python scripts/hyp_search.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd1df07",
   "metadata": {},
   "source": [
    "### 4.2 Small Grid Search Example\n",
    "\n",
    "Let's run a small grid search with just a few experiments to test the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a small grid search (limited to 4 experiments for testing)\n",
    "!python scripts/hyp_search.py \\\n",
    "    --max_experiments 4 \\\n",
    "    --fast_search \\\n",
    "    --output_dir test_grid_search \\\n",
    "    --device auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c1d69",
   "metadata": {},
   "source": [
    "### 4.3 Full Grid Search\n",
    "\n",
    "**Warning**: The full grid search will run 48 experiments.\n",
    "\n",
    "Uncomment and run the cell below only if you want to perform the complete hyperparameter search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full grid search (48 experiments) - UNCOMMENT TO RUN\n",
    "# This will take several hours to complete!\n",
    "\n",
    "# !python scripts/hyp_search.py \\\n",
    "#     --fast_search \\\n",
    "#     --output_dir full_grid_search \\\n",
    "#     --device auto\n",
    "\n",
    "print(\"Full grid search is commented out. Uncomment the above lines to run all 48 experiments.\")\n",
    "print(\"\\nGrid search configuration:\")\n",
    "print(\"- hidden_channels: [32, 128]\")\n",
    "print(\"- num_layers: [3, 4]\")\n",
    "print(\"- dropout: [0.2, 0.5]\")\n",
    "print(\"- batch_size: [32, 64]\")\n",
    "print(\"- lr: [0.01, 0.005, 0.001]\")\n",
    "print(\"- Total combinations: 2 × 2 × 2 × 2 × 3 = 48 experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2212263",
   "metadata": {},
   "source": [
    "### 4.4 Grid Search Results Analysis\n",
    "\n",
    "After running grid search, let's analyze the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5fcf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_display_grid_search_results(results_dir=\"test_grid_search\"):\n",
    "    \"\"\"\n",
    "    Load and display grid search results from JSON file.\n",
    "    \"\"\"\n",
    "    results_file = Path(results_dir) / \"grid_search_results.json\"\n",
    "    \n",
    "    if not results_file.exists():\n",
    "        print(f\"Results file not found: {results_file}\")\n",
    "        print(\"Run the grid search first!\")\n",
    "        return None\n",
    "    \n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"GRID SEARCH RESULTS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    info = results['grid_search_info']\n",
    "    print(f\"Timestamp: {info['timestamp']}\")\n",
    "    print(f\"Total experiments: {info['total_experiments']}\")\n",
    "    print(f\"Successful: {info['successful_experiments']}\")\n",
    "    print(f\"Failed: {info['failed_experiments']}\")\n",
    "    print(f\"Total time: {info['total_time_hours']:.2f} hours ({info['total_time_minutes']:.1f} minutes)\")\n",
    "    print(f\"Fast search mode: {info['fast_search_mode']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"TOP RESULTS:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    for i, result in enumerate(results['best_results'][:5], 1):\n",
    "        hp = result['hyperparameters']\n",
    "        print(f\"{i}. Experiment {result['experiment_id']}:\")\n",
    "        print(f\"   ROC AUC: {result['val_roc_auc']:.4f} (test: {result['test_roc_auc']:.4f})\")\n",
    "        print(f\"   Config: hidden={hp['hidden_channels']}, layers={hp['num_layers']}, \"\n",
    "              f\"dropout={hp['dropout']}, batch={hp['batch_size']}, lr={hp['lr']}\")\n",
    "        print(f\"   Time: {result['experiment_time_minutes']:.1f}min, \"\n",
    "              f\"Epochs: {result['total_epochs']}/{result.get('planned_epochs', 'N/A')}\")\n",
    "        if result.get('early_stopped', False):\n",
    "            print(f\"   Early stopped at epoch {result['best_epoch']}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load and display results\n",
    "results = load_and_display_grid_search_results(\"test_grid_search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2cc620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grid_search_results(results):\n",
    "    \"\"\"\n",
    "    Create visualizations for grid search results.\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        print(\"No results to visualize. Run grid search first.\")\n",
    "        return\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    experiments = results['all_experiments']\n",
    "    successful_experiments = [exp for exp in experiments if exp['success']]\n",
    "    \n",
    "    if not successful_experiments:\n",
    "        print(\"No successful experiments to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare data\n",
    "    val_roc_aucs = []\n",
    "    test_roc_aucs = []\n",
    "    experiment_times = []\n",
    "    total_epochs = []\n",
    "    hyperparams = []\n",
    "    \n",
    "    for exp in successful_experiments:\n",
    "        if 'results' in exp and 'final_results' in exp['results']:\n",
    "            # Get validation ROC AUC from training info\n",
    "            val_roc = exp['results']['training_info']['best_val_roc_auc']\n",
    "            # Get test ROC AUC from final results\n",
    "            test_roc = exp['results']['final_results']['test']['roc_auc']\n",
    "            \n",
    "            val_roc_aucs.append(val_roc)\n",
    "            test_roc_aucs.append(test_roc)\n",
    "            experiment_times.append(exp['experiment_time_minutes'])\n",
    "            total_epochs.append(exp['results']['training_info']['total_epochs'])\n",
    "            \n",
    "            # Store hyperparameters\n",
    "            hp = exp['hyperparameters']\n",
    "            hyperparams.append(f\"H{hp['hidden_channels']}_L{hp['num_layers']}_D{hp['dropout']}_B{hp['batch_size']}_LR{hp['lr']}\")\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Validation vs Test ROC AUC\n",
    "    axes[0, 0].scatter(val_roc_aucs, test_roc_aucs, alpha=0.7, s=100, c='blue')\n",
    "    axes[0, 0].plot([0.5, 1.0], [0.5, 1.0], 'r--', alpha=0.5, label='Perfect correlation')\n",
    "    axes[0, 0].set_xlabel('Validation ROC AUC')\n",
    "    axes[0, 0].set_ylabel('Test ROC AUC')\n",
    "    axes[0, 0].set_title('Validation vs Test Performance')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    corr = np.corrcoef(val_roc_aucs, test_roc_aucs)[0, 1]\n",
    "    axes[0, 0].text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=axes[0, 0].transAxes, \n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # Plot 2: ROC AUC vs Training Time\n",
    "    axes[0, 1].scatter(experiment_times, val_roc_aucs, alpha=0.7, s=100, c='green')\n",
    "    axes[0, 1].set_xlabel('Training Time (minutes)')\n",
    "    axes[0, 1].set_ylabel('Validation ROC AUC')\n",
    "    axes[0, 1].set_title('Performance vs Training Time')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: ROC AUC vs Epochs\n",
    "    axes[1, 0].scatter(total_epochs, val_roc_aucs, alpha=0.7, s=100, c='purple')\n",
    "    axes[1, 0].set_xlabel('Total Epochs Trained')\n",
    "    axes[1, 0].set_ylabel('Validation ROC AUC')\n",
    "    axes[1, 0].set_title('Performance vs Training Epochs')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Performance distribution\n",
    "    axes[1, 1].hist(val_roc_aucs, bins=min(10, len(val_roc_aucs)), alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1, 1].axvline(np.mean(val_roc_aucs), color='red', linestyle='--', label=f'Mean: {np.mean(val_roc_aucs):.3f}')\n",
    "    axes[1, 1].axvline(np.median(val_roc_aucs), color='blue', linestyle='--', label=f'Median: {np.median(val_roc_aucs):.3f}')\n",
    "    axes[1, 1].set_xlabel('Validation ROC AUC')\n",
    "    axes[1, 1].set_ylabel('Number of Experiments')\n",
    "    axes[1, 1].set_title('Performance Distribution')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n=== PERFORMANCE STATISTICS ===\")\n",
    "    print(f\"Best validation ROC AUC: {max(val_roc_aucs):.4f}\")\n",
    "    print(f\"Worst validation ROC AUC: {min(val_roc_aucs):.4f}\")\n",
    "    print(f\"Mean validation ROC AUC: {np.mean(val_roc_aucs):.4f} ± {np.std(val_roc_aucs):.4f}\")\n",
    "    print(f\"Median validation ROC AUC: {np.median(val_roc_aucs):.4f}\")\n",
    "    print(f\"\\nMean training time: {np.mean(experiment_times):.1f} ± {np.std(experiment_times):.1f} minutes\")\n",
    "    print(f\"Mean epochs: {np.mean(total_epochs):.1f} ± {np.std(total_epochs):.1f}\")\n",
    "\n",
    "# Visualize results if available\n",
    "if results:\n",
    "    visualize_grid_search_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2878e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hyperparameter_impact(results):\n",
    "    \"\"\"\n",
    "    Analyze the impact of different hyperparameters on model performance.\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        print(\"No results to analyze. Run grid search first.\")\n",
    "        return\n",
    "    \n",
    "    experiments = results['all_experiments']\n",
    "    successful_experiments = [exp for exp in experiments if exp['success']]\n",
    "    \n",
    "    if not successful_experiments:\n",
    "        print(\"No successful experiments to analyze.\")\n",
    "        return\n",
    "    \n",
    "    # Collect data\n",
    "    data = []\n",
    "    for exp in successful_experiments:\n",
    "        if 'results' in exp and 'training_info' in exp['results']:\n",
    "            hp = exp['hyperparameters']\n",
    "            performance = exp['results']['training_info']['best_val_roc_auc']\n",
    "            \n",
    "            data.append({\n",
    "                'hidden_channels': hp['hidden_channels'],\n",
    "                'num_layers': hp['num_layers'],\n",
    "                'dropout': hp['dropout'],\n",
    "                'batch_size': hp['batch_size'],\n",
    "                'lr': hp['lr'],\n",
    "                'val_roc_auc': performance\n",
    "            })\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No valid data found for analysis.\")\n",
    "        return\n",
    "    \n",
    "    # Convert to arrays for analysis\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"\\n=== HYPERPARAMETER IMPACT ANALYSIS ===\")\n",
    "    \n",
    "    # Analyze each hyperparameter\n",
    "    hyperparams = ['hidden_channels', 'num_layers', 'dropout', 'batch_size', 'lr']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, param in enumerate(hyperparams):\n",
    "        # Group by parameter value and calculate mean performance\n",
    "        grouped = df.groupby(param)['val_roc_auc'].agg(['mean', 'std', 'count']).reset_index()\n",
    "        \n",
    "        # Plot\n",
    "        ax = axes[i]\n",
    "        ax.bar(grouped[param].astype(str), grouped['mean'], \n",
    "               yerr=grouped['std'], capsize=5, alpha=0.7)\n",
    "        ax.set_xlabel(param)\n",
    "        ax.set_ylabel('Mean Validation ROC AUC')\n",
    "        ax.set_title(f'Impact of {param}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for j, (val, mean_val, count) in enumerate(zip(grouped[param], grouped['mean'], grouped['count'])):\n",
    "            ax.text(j, mean_val + 0.01, f'{mean_val:.3f}\\n(n={count})', \n",
    "                   ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"\\n{param.upper()}:\")\n",
    "        for _, row in grouped.iterrows():\n",
    "            print(f\"  {row[param]}: {row['mean']:.4f} ± {row['std']:.4f} (n={row['count']})\")\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    axes[-1].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best hyperparameter combinations\n",
    "    print(\"\\n=== BEST HYPERPARAMETER COMBINATIONS ===\")\n",
    "    top_experiments = df.nlargest(3, 'val_roc_auc')\n",
    "    \n",
    "    for i, (_, exp) in enumerate(top_experiments.iterrows(), 1):\n",
    "        print(f\"{i}. ROC AUC: {exp['val_roc_auc']:.4f}\")\n",
    "        print(f\"   Config: hidden={exp['hidden_channels']}, layers={exp['num_layers']}, \"\n",
    "              f\"dropout={exp['dropout']}, batch={exp['batch_size']}, lr={exp['lr']}\")\n",
    "\n",
    "# Analyze hyperparameter impact if results are available\n",
    "if results:\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        analyze_hyperparameter_impact(results)\n",
    "    except ImportError:\n",
    "        print(\"pandas not available. Install with: pip install pandas\")\n",
    "        print(\"Skipping hyperparameter analysis...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688dc20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Conclusion and Next Steps\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Environment setup** for both local and Google Colab environments\n",
    "2. **Basic model training** with customizable hyperparameters\n",
    "3. **Systematic grid search** for hyperparameter optimization\n",
    "4. **Comprehensive analysis** of results and hyperparameter impact\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- The grid search explores 48 hyperparameter combinations\n",
    "- Fast search mode with early stopping significantly reduces training time\n",
    "- Results are automatically saved and can be analyzed systematically\n",
    "\n",
    "### Useful Commands\n",
    "\n",
    "```bash\n",
    "# Single training run\n",
    "python scripts/train.py --epochs 100 --lr 1e-3 --hidden_channels 128\n",
    "\n",
    "# Grid search (fast mode)\n",
    "python scripts/hyp_search.py --fast_search --max_experiments 10\n",
    "\n",
    "# Full grid search\n",
    "python scripts/hyp_search.py --fast_search\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
