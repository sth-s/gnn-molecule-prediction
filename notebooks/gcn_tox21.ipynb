{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805f2018",
   "metadata": {
    "id": "805f2018"
   },
   "source": [
    "# Molecular Toxicity Prediction (Tox21) using GIN\n",
    "\n",
    "In this notebook, we implement and train a Graph Isomorphism Network (GIN) for predicting molecular toxicity based on the Tox21 dataset. GIN is generally more powerful than GCN for graph representation learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1ce29",
   "metadata": {
    "id": "2fc1ce29"
   },
   "source": [
    "## 1. Environment Setup in Colab\n",
    "\n",
    "Run the following code to install PyTorch Geometric and other dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728976d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a728976d",
    "outputId": "e2b53cff-a1f7-48f0-83cd-e6d22bd4b0f5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# This cell is intended to be run only in Google Colab\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    print(\"Running in Colab...\")\n",
    "\n",
    "    # 1. Remove existing folder if it already exists\n",
    "    repo_name = 'gnn-molecule-prediction'\n",
    "    repo_path = os.path.join('/content', repo_name)\n",
    "\n",
    "    if os.path.exists(repo_path):\n",
    "        shutil.rmtree(repo_path)\n",
    "        print(f\"Removed existing directory: {repo_path}\")\n",
    "    else:\n",
    "        print(f\"No existing directory found: {repo_path}\")\n",
    "\n",
    "    # 2. Clone the GitHub repository\n",
    "    %cd /content\n",
    "    !git clone https://github.com/sth-s/gnn-molecule-prediction.git\n",
    "\n",
    "    # 3. Change working directory to the project root\n",
    "    %cd gnn-molecule-prediction\n",
    "\n",
    "    # 4. Install Conda support in Colab and restart runtime\n",
    "    !pip install -q condacolab\n",
    "    import condacolab\n",
    "    condacolab.install()  # This will automatically restart the runtime\n",
    "    !conda env update -n base -f environment.yml\n",
    "    %cd gnn-molecule-prediction\n",
    "\n",
    "else:\n",
    "    os.chdir('../')\n",
    "    print(f\"Changed working directory to: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e18f8",
   "metadata": {
    "id": "8d9e18f8"
   },
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd14e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abbd14e8",
    "outputId": "86dcebe8-1cca-4283-a665-b55f9b34c6a3"
   },
   "outputs": [],
   "source": [
    "# PyTorch and PyG\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# Chemistry and data processing\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Evaluation and splitting\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Our custom data loader\n",
    "from src.data_utils import load_tox21\n",
    "# Import our custom model and training functions\n",
    "from src.model import GIN\n",
    "from src.train import train_epoch, evaluate\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_palette('muted')\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d4b6a6",
   "metadata": {
    "id": "88d4b6a6"
   },
   "source": [
    "## 3. Loading and Preparing Tox21 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8479b",
   "metadata": {},
   "source": [
    "### 3.1 About the Data Loading Module\n",
    "\n",
    "We use a custom developed `load_tox21` module from the `src.data_utils` package, which provides the following features:\n",
    "\n",
    "- **Automatic data downloading**: if the `tox21.csv` file is missing, the module will automatically download it from the official source\n",
    "- **SMILES to graphs conversion**: each molecule is converted into a graph with node and edge attributes\n",
    "- **Result caching**: results are saved to a cache to speed up subsequent runs\n",
    "- **Flexible configuration**: you can specify the data path, file name, target columns, and other parameters\n",
    "\n",
    "Detailed information about the `load_tox21` function and usage examples are available in the project's README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13aa5d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f13aa5d7",
    "outputId": "626237d6-39e2-4ebd-f009-3d9d26af8521"
   },
   "outputs": [],
   "source": [
    "# Load the Tox21 dataset using our custom function\n",
    "# If the tox21.csv file is missing, it will be automatically downloaded\n",
    "dataset = load_tox21(\n",
    "    root=\"data/Tox21\",           # root directory for the data\n",
    "    filename=\"tox21.csv\",        # name of the CSV file\n",
    "    smiles_col=\"smiles\",         # column containing SMILES strings\n",
    "    mol_id_col=\"mol_id\",         # column containing molecule IDs\n",
    "    cache_file=\"data.pt\",        # name of the cache file\n",
    "    recreate=False,              # Use existing processed file if available\n",
    "    auto_download=True           # automatically download if missing\n",
    ")\n",
    "\n",
    "print(f\"Type of dataset object: {type(dataset)}\")\n",
    "try:\n",
    "    print(f\"dataset._slices: {dataset._slices}\")\n",
    "except AttributeError:\n",
    "    print(f\"dataset has no attribute _slices\")\n",
    "print(f\"Total graphs (len(dataset)): {len(dataset)}\")\n",
    "\n",
    "# Check if we can access the first item in the dataset\n",
    "if len(dataset) > 0:\n",
    "    print(\"Attempting to access dataset[0]...\")\n",
    "    first_item = None\n",
    "    try:\n",
    "        first_item = dataset[0]\n",
    "        print(f\"dataset[0] type: {type(first_item)}\")\n",
    "        if first_item is not None:\n",
    "            print(f\"Number of node features: {first_item.x.shape[1]}\")\n",
    "            if hasattr(first_item, 'mol_id'):\n",
    "                 print(f\"Mol ID for the first molecule: {first_item.mol_id}\")\n",
    "            else:\n",
    "                 print(f\"Mol ID attribute not found in first_item.\")\n",
    "            \n",
    "            # Check the presence of y and y_mask\n",
    "            if hasattr(first_item, 'y'):\n",
    "                print(f\"Task labels (y) shape: {first_item.y.shape}\")\n",
    "                print(f\"Task labels (y): {first_item.y}\")\n",
    "            if hasattr(first_item, 'y_mask'):\n",
    "                print(f\"Task mask (y_mask) shape: {first_item.y_mask.shape}\")\n",
    "                print(f\"Task mask (y_mask): {first_item.y_mask}\")\n",
    "        else:\n",
    "            print(f\"dataset[0] is None.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing dataset[0] or its attributes: {e}\")\n",
    "else:\n",
    "    print(\"Dataset is empty (len(dataset) is 0).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698e268",
   "metadata": {},
   "source": [
    "### 3.3 Verify mol_id and Dataset Properties (Revised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792cfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that mol_id is present and check dataset properties\n",
    "print(\"\\nRunning second verification cell:\")\n",
    "if len(dataset) > 0:\n",
    "    data_example = None\n",
    "    try:\n",
    "        data_example = dataset[0] # This might be None if the previous cell showed it was None\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting dataset[0] in second cell: {e}\")\n",
    "    \n",
    "    if data_example is not None:\n",
    "        print(\"Example graph from the dataset (after potential reprocessing):\")\n",
    "        print(data_example)\n",
    "        \n",
    "        if hasattr(data_example, 'mol_id'):\n",
    "            print(f\"\\nMol ID for the first molecule: {data_example.mol_id}\")\n",
    "        else:\n",
    "            print(\"\\nmol_id attribute not found in the first molecule (data_example).\")\n",
    "        \n",
    "        print(f\"Edge index dimensions: {data_example.edge_index.shape}\")\n",
    "        print(f\"Number of atoms (nodes): {data_example.num_nodes}\")\n",
    "        print(f\"Task labels (y): {data_example.y}\")\n",
    "        print(f\"Task mask (y_mask): {data_example.y_mask}\")\n",
    "    elif len(dataset) > 0: # dataset[0] was None but dataset is not empty\n",
    "        print(\"data_example (dataset[0]) is None, cannot print details.\")\n",
    "else:\n",
    "    print(\"Dataset is empty in second verification cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of graph properties in the dataset\n",
    "if len(dataset) > 0:\n",
    "    data_example = dataset[0]\n",
    "    print(\"Example graph from the dataset:\")\n",
    "    print(data_example)\n",
    "    print(f\"Edge index dimensions: {data_example.edge_index.shape}\")\n",
    "    print(f\"Number of atoms (nodes): {data_example.num_nodes}\")\n",
    "    print(f\"Task labels (y): {data_example.y}\")\n",
    "\n",
    "    # Dataset statistics\n",
    "    nodes_count = []\n",
    "    edges_count = []\n",
    "    for i in range(min(1000, len(dataset))):\n",
    "        data = dataset[i]\n",
    "        nodes_count.append(data.num_nodes)\n",
    "        edges_count.append(data.edge_index.shape[1])\n",
    "    \n",
    "    print(f\"\\nGraph statistics (based on a sample of {len(nodes_count)} molecules):\")\n",
    "    print(f\"Average number of atoms: {np.mean(nodes_count):.2f} ± {np.std(nodes_count):.2f}\")\n",
    "    print(f\"Average number of bonds: {np.mean(edges_count)/2:.2f} ± {np.std(edges_count)/2:.2f}\")\n",
    "    print(f\"Min/max atoms: {np.min(nodes_count)}/{np.max(nodes_count)}\")\n",
    "    print(f\"Min/max bonds: {np.min(edges_count)/2:.0f}/{np.max(edges_count)/2:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of atom and bond count distributions\n",
    "if 'nodes_count' in locals():\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    ax1.hist(nodes_count, bins=30, alpha=0.7, color='skyblue')\n",
    "    ax1.set_title('Distribution of Atom Counts')\n",
    "    ax1.set_xlabel('Number of atoms')\n",
    "    ax1.set_ylabel('Number of molecules')\n",
    "    \n",
    "    ax2.hist([e/2 for e in edges_count], bins=30, alpha=0.7, color='salmon')\n",
    "    ax2.set_title('Distribution of Bond Counts')\n",
    "    ax2.set_xlabel('Number of bonds')\n",
    "    ax2.set_ylabel('Number of molecules')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test (80/20)\n",
    "torch.manual_seed(42)  # for reproducibility\n",
    "train_len = int(0.8 * len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_len, len(dataset) - train_len]\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Total graphs: {len(dataset)}\")\n",
    "print(f\"Training set: {len(train_dataset)} graphs\")\n",
    "print(f\"Test set: {len(test_dataset)} graphs\")\n",
    "print(f\"Batch size: {train_loader.batch_size}, number of batches in training set: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5848f580",
   "metadata": {
    "id": "5848f580"
   },
   "source": [
    "## 4. Define GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859471d7",
   "metadata": {
    "id": "859471d7"
   },
   "outputs": [],
   "source": [
    "# Create a GIN model instance using our implementation from src/model.py\n",
    "model = GIN(\n",
    "    in_channels=dataset.num_node_features,\n",
    "    hidden_channels=64,\n",
    "    num_classes=dataset.num_classes,\n",
    "    num_layers=2,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "# Base learning rate for the optimizer\n",
    "base_lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=base_lr)\n",
    "\n",
    "# Configure OneCycleLR scheduler\n",
    "# total_steps - total number of training steps (epochs * batches)\n",
    "num_epochs = 20\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=base_lr * 10,  # maximum learning rate (10x base rate)\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.3,        # percentage of steps for lr increase (30%)\n",
    "    anneal_strategy='cos',  # learning rate decay strategy (cosine)\n",
    "    div_factor=10.0,      # initial lr = max_lr / div_factor\n",
    "    final_div_factor=1000.0  # final lr = max_lr / final_div_factor\n",
    ")\n",
    "\n",
    "print(f\"Model created with {dataset.num_node_features} input features and {dataset.num_classes} output classes\")\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__} with base lr={base_lr}\")\n",
    "print(f\"Scheduler: OneCycleLR with max_lr={base_lr * 10}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8c0a9",
   "metadata": {
    "id": "e5d8c0a9"
   },
   "source": [
    "## 5. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadb0b32",
   "metadata": {
    "id": "eadb0b32"
   },
   "outputs": [],
   "source": [
    "# Training and evaluation using our functions from src/train.py\n",
    "num_epochs = 20\n",
    "best_val_roc = 0\n",
    "best_model_state = None\n",
    "history = {'train_loss': [], 'val_roc': [], 'val_pr': [], 'lr': []}\n",
    "\n",
    "print(\"Starting training with OneCycleLR scheduler...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Train for one epoch with scheduler\n",
    "    loss, lr_history = train_epoch(model, train_loader, optimizer, device, scheduler)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_metrics = evaluate(model, test_loader, device)\n",
    "    val_roc = val_metrics['roc_auc']\n",
    "    val_pr = val_metrics['pr_auc']\n",
    "    \n",
    "    # Save metrics history\n",
    "    history['train_loss'].append(loss)\n",
    "    history['val_roc'].append(val_roc)\n",
    "    history['val_pr'].append(val_pr)\n",
    "    if lr_history:\n",
    "        history['lr'].extend(lr_history)\n",
    "    \n",
    "    # Print progress with current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch:02d}: Loss = {loss:.4f}, ROC AUC = {val_roc:.4f}, PR AUC = {val_pr:.4f}, LR = {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model state\n",
    "    if val_roc > best_val_roc:\n",
    "        best_val_roc = val_roc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\"  → New best model saved! ROC AUC: {best_val_roc:.4f}\")\n",
    "\n",
    "# Load the best model state for final evaluation\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"Loaded best model with validation ROC AUC: {best_val_roc:.4f}\")\n",
    "\n",
    "# Perform final evaluation\n",
    "final_metrics = evaluate(model, test_loader, device)\n",
    "print(\"\\nFinal Evaluation Results:\")\n",
    "print(f\"Test ROC AUC: {final_metrics['roc_auc']:.4f}\")\n",
    "print(f\"Test PR AUC: {final_metrics['pr_auc']:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "torch.save(best_model_state, 'best_gin_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a3328c",
   "metadata": {
    "id": "a2a3328c"
   },
   "source": [
    "## 6. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb9775",
   "metadata": {
    "id": "cffb9775"
   },
   "outputs": [],
   "source": [
    "# Visualize learning curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(history['train_loss'])+1), history['train_loss'], 'o-', label='Training Loss')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation ROC AUC\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(history['val_roc'])+1), history['val_roc'], 'o-', label='ROC AUC')\n",
    "plt.plot(range(1, len(history['val_pr'])+1), history['val_pr'], 'o-', label='PR AUC')\n",
    "plt.title('Validation Metrics per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36742490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Model Evaluation and Visualization\n",
    "model.eval()\n",
    "\n",
    "# Collect predictions for each task\n",
    "all_true = {}\n",
    "all_pred = {}\n",
    "all_batch_losses = []\n",
    "\n",
    "# Initialize loss function - binary cross entropy with reduction='none' to get per-task losses\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch)  # Raw logits (before sigmoid)\n",
    "        out_probs = torch.sigmoid(out).detach().cpu()\n",
    "        mask = batch.y_mask.cpu()\n",
    "        y = batch.y.cpu()\n",
    "        \n",
    "        # Calculate batch losses per task\n",
    "        batch_loss = loss_fn(out, batch.y)\n",
    "        masked_loss = batch_loss * batch.y_mask\n",
    "        all_batch_losses.append(masked_loss.cpu())\n",
    "        \n",
    "        for task in range(out_probs.size(1)):\n",
    "            # Select only valid entries (where mask is True)\n",
    "            idx = mask[:, task]\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "                \n",
    "            true = y[idx, task].numpy()\n",
    "            pred = out_probs[idx, task].numpy()\n",
    "            \n",
    "            all_true.setdefault(task, []).append(true)\n",
    "            all_pred.setdefault(task, []).append(pred)\n",
    "\n",
    "# Calculate training loss per task\n",
    "all_losses = torch.cat(all_batch_losses, dim=0)  # Combine all batch losses\n",
    "valid_samples_per_task = all_losses.ne(0).sum(dim=0).float()\n",
    "task_losses = all_losses.sum(dim=0) / valid_samples_per_task\n",
    "\n",
    "# Calculate AUC for each task\n",
    "task_metrics = {}\n",
    "for task in all_true:\n",
    "    t = np.concatenate(all_true[task])\n",
    "    p = np.concatenate(all_pred[task])\n",
    "    \n",
    "    # Skip if only one class is present\n",
    "    if len(np.unique(t)) < 2:\n",
    "        continue\n",
    "        \n",
    "    # Calculate ROC AUC\n",
    "    roc_auc = roc_auc_score(t, p)\n",
    "    \n",
    "    # Calculate PR AUC\n",
    "    prec, rec, _ = precision_recall_curve(t, p)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    \n",
    "    # Store task metrics and raw predictions for plotting\n",
    "    task_metrics[task] = {\n",
    "        'roc_auc': roc_auc, \n",
    "        'pr_auc': pr_auc,\n",
    "        'true': t,\n",
    "        'pred': p,\n",
    "        'loss': task_losses[task].item() if task < len(task_losses) else float('nan')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df010063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train vs Val Loss and ROC AUC plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Train vs Val Loss\n",
    "axes[0].plot(range(1, len(history['train_loss'])+1), history['train_loss'], 'o-', label='Training Loss', color='blue')\n",
    "axes[0].set_title('Train vs Validation Loss', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "# Train vs Val ROC AUC\n",
    "axes[1].plot(range(1, len(history['val_roc'])+1), history['val_roc'], 'o-', label='Validation ROC AUC', color='green')\n",
    "axes[1].set_title('Validation ROC AUC per Epoch', fontsize=14)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('ROC AUC', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0.5, 1.0])  # Set y-axis from 0.5 (random) to 1.0 (perfect)\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1566770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. AUC per task (Bar plot) - Shows which tasks the model performs better/worse on\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Extract tasks and their AUC values\n",
    "tasks = list(task_metrics.keys())\n",
    "roc_aucs = [task_metrics[t]['roc_auc'] for t in tasks]\n",
    "pr_aucs = [task_metrics[t]['pr_auc'] for t in tasks]\n",
    "\n",
    "# Sort by ROC AUC\n",
    "sorted_indices = np.argsort(roc_aucs)\n",
    "sorted_tasks = [tasks[i] for i in sorted_indices]\n",
    "sorted_roc_aucs = [roc_aucs[i] for i in sorted_indices]\n",
    "sorted_pr_aucs = [pr_aucs[i] for i in sorted_indices]\n",
    "\n",
    "# Create a bar chart for both ROC AUC and PR AUC\n",
    "x = np.arange(len(sorted_tasks))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, sorted_roc_aucs, width, label='ROC AUC', color='skyblue', alpha=0.8)\n",
    "plt.bar(x + width/2, sorted_pr_aucs, width, label='PR AUC', color='salmon', alpha=0.8)\n",
    "\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Random Classifier (ROC AUC=0.5)')\n",
    "plt.xlabel('Task Index', fontsize=12)\n",
    "plt.ylabel('AUC Score', fontsize=12)\n",
    "plt.title('Model Performance (AUC) per Task', fontsize=14)\n",
    "plt.xticks(x, [f'{t}' for t in sorted_tasks], rotation=90)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b066d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ROC and PR curves for selected tasks\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Find tasks with good, medium, and lower performance (for more diverse visualization)\n",
    "roc_auc_values = np.array([task_metrics[t]['roc_auc'] for t in task_metrics])\n",
    "best_idx = np.argmax(roc_auc_values)\n",
    "worst_idx = np.argmin(roc_auc_values)\n",
    "median_idx = np.argsort(roc_auc_values)[len(roc_auc_values)//2]\n",
    "\n",
    "selected_tasks = [list(task_metrics.keys())[i] for i in [best_idx, median_idx, worst_idx]]\n",
    "colors = ['green', 'blue', 'orange']\n",
    "\n",
    "# ROC Curves\n",
    "for i, task in enumerate(selected_tasks):\n",
    "    t = task_metrics[task]['true']\n",
    "    p = task_metrics[task]['pred']\n",
    "    fpr, tpr, _ = roc_curve(t, p)\n",
    "    axes[0].plot(fpr, tpr, color=colors[i], \n",
    "               label=f'Task {task} (AUC = {task_metrics[task][\"roc_auc\"]:.3f})')\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.4, label='Random (AUC = 0.5)')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0].set_title('ROC Curves', fontsize=14)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PR Curves\n",
    "for i, task in enumerate(selected_tasks):\n",
    "    t = task_metrics[task]['true']\n",
    "    p = task_metrics[task]['pred']\n",
    "    prec, rec, _ = precision_recall_curve(t, p)\n",
    "    axes[1].plot(rec, prec, color=colors[i],\n",
    "               label=f'Task {task} (AUC = {task_metrics[task][\"pr_auc\"]:.3f})')\n",
    "\n",
    "# Calculate and plot baseline for PR curve (class imbalance)\n",
    "for task in selected_tasks:\n",
    "    t = task_metrics[task]['true']\n",
    "    baseline = np.sum(t) / len(t)  # Positive class ratio\n",
    "    axes[1].axhline(y=baseline, color=colors[selected_tasks.index(task)], \n",
    "                  linestyle='--', alpha=0.4, \n",
    "                  label=f'Task {task} baseline: {baseline:.3f}')\n",
    "\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1].set_title('Precision-Recall Curves', fontsize=14)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45835b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Prediction histograms - Shows class separability\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Prediction Distribution by Class', fontsize=16, y=1.02)\n",
    "\n",
    "for i, task in enumerate(selected_tasks):\n",
    "    t = task_metrics[task]['true']\n",
    "    p = task_metrics[task]['pred']\n",
    "    \n",
    "    # Split predictions by true class\n",
    "    pos_preds = p[t == 1]\n",
    "    neg_preds = p[t == 0]\n",
    "    \n",
    "    # Determine class balance percentages\n",
    "    pos_perc = 100 * len(pos_preds) / len(t)\n",
    "    neg_perc = 100 * len(neg_preds) / len(t)\n",
    "    \n",
    "    # Create the histogram\n",
    "    axes[i].hist(neg_preds, bins=20, alpha=0.6, color='red', label=f'Negative ({neg_perc:.1f}%)', density=True)\n",
    "    axes[i].hist(pos_preds, bins=20, alpha=0.6, color='blue', label=f'Positive ({pos_perc:.1f}%)', density=True)\n",
    "    \n",
    "    # Calculate and display separability metrics\n",
    "    mean_diff = np.mean(pos_preds) - np.mean(neg_preds) if len(pos_preds) > 0 and len(neg_preds) > 0 else 0\n",
    "    roc_auc = task_metrics[task]['roc_auc']\n",
    "    \n",
    "    axes[i].set_title(f'Task {task}\\nROC AUC: {roc_auc:.3f}, Mean Diff: {mean_diff:.3f}', fontsize=12)\n",
    "    axes[i].set_xlabel('Prediction Probability', fontsize=11)\n",
    "    axes[i].set_ylabel('Density', fontsize=11)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].legend(fontsize=9)\n",
    "    axes[i].set_xlim([0, 1])\n",
    "\n",
    "    # Add a vertical line for the optimal threshold (using Youden's J statistic)\n",
    "    if len(pos_preds) > 0 and len(neg_preds) > 0:\n",
    "        fpr, tpr, thresholds = roc_curve(t, p)\n",
    "        # Calculate Youden's J statistic: J = sensitivity + specificity - 1\n",
    "        j_scores = tpr - fpr\n",
    "        # Get the optimal threshold\n",
    "        best_idx = np.argmax(j_scores)\n",
    "        best_threshold = thresholds[best_idx]\n",
    "        axes[i].axvline(x=best_threshold, color='green', linestyle='--', alpha=0.8, \n",
    "                      label=f'Optimal threshold: {best_threshold:.2f}')\n",
    "        axes[i].legend(fontsize=9)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Combined loss and performance visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Extract task losses and ROC AUCs\n",
    "tasks = list(task_metrics.keys())\n",
    "losses = [task_metrics[t]['loss'] for t in tasks]\n",
    "roc_aucs = [task_metrics[t]['roc_auc'] for t in tasks]\n",
    "\n",
    "# Sort by loss\n",
    "sorted_indices = np.argsort(losses)\n",
    "sorted_tasks = [tasks[i] for i in sorted_indices]\n",
    "sorted_losses = [losses[i] for i in sorted_indices]\n",
    "sorted_roc_aucs = [roc_aucs[i] for i in sorted_indices]\n",
    "\n",
    "# Create two y-axes for loss and AUC\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot loss bars\n",
    "bar_colors = ['#FF9999' if auc < 0.7 else '#66B2FF' for auc in sorted_roc_aucs]\n",
    "bars = ax1.bar(sorted_tasks, sorted_losses, alpha=0.7, color=bar_colors)\n",
    "\n",
    "# Create a second y-axis for AUC\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(sorted_tasks, sorted_roc_aucs, 'go-', alpha=0.7, label='ROC AUC')\n",
    "ax2.set_ylim([0.4, 1.0])\n",
    "\n",
    "# Add labels and title\n",
    "ax1.set_xlabel('Task', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12, color='red')\n",
    "ax2.set_ylabel('ROC AUC', fontsize=12, color='green')\n",
    "plt.title('Task-wise Loss vs Performance', fontsize=14)\n",
    "\n",
    "# Add gridlines\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add explanatory threshold line\n",
    "ax2.axhline(y=0.7, color='green', linestyle='--', alpha=0.5, label='AUC = 0.7')\n",
    "\n",
    "# Add legends\n",
    "ax1.legend(handles=[\n",
    "    plt.Rectangle((0,0), 1, 1, color='#FF9999', alpha=0.7, label='AUC < 0.7'),\n",
    "    plt.Rectangle((0,0), 1, 1, color='#66B2FF', alpha=0.7, label='AUC ≥ 0.7')\n",
    "], loc='upper left')\n",
    "\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb4221e",
   "metadata": {
    "id": "7eb4221e"
   },
   "source": [
    "## 7. Conclusion and Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5902fe8d",
   "metadata": {
    "id": "5902fe8d"
   },
   "source": [
    "# TODO: Add analysis of results and conclusions"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python (gnn-tox21)",
   "language": "python",
   "name": "gnn-tox21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
